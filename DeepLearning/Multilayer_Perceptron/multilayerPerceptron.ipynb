{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9880364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25fd492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbe64b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(BaseLayer):\n",
    "    def __init__(self, input_sizes, output_sizes):\n",
    "        super().__init__()\n",
    "        self.weights = np.random.randn(output_sizes, input_sizes)\n",
    "        self.bias = np.random.randn(output_sizes, 1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, outputGradient, learningRate):\n",
    "        inputGradient = np.dot(self.weights.transpose(), outputGradient)\n",
    "        weightGradient = np.dot(outputGradient, self.input.transpose())\n",
    "        self.weights = self.weights - learningRate*weightGradient\n",
    "        self.bias = self.bias - learningRate*outputGradient\n",
    "        return inputGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8df8e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(BaseLayer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(input)\n",
    "    \n",
    "    def backward(self, outputGradient, learning_rate):\n",
    "        return np.multiply(outputGradient, self.activation_prime(self.input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e8ab75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(ActivationLayer):\n",
    "    def __init__(self):\n",
    "        activation = lambda x : np.tanh(x)\n",
    "        activation_prime = lambda x : 1 - np.tanh(x)**2\n",
    "        super().__init__(activation, activation_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83ace238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSquaredError(yPred, yTrue):\n",
    "    return np.mean(np.power(yPred - yTrue, 2))\n",
    "\n",
    "def meanSquaredErrorPrime(yPred , yTrue):\n",
    "    return 2 * (yPred - yTrue) / np.size(yTrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0a89f",
   "metadata": {},
   "source": [
    "### XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fedee0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, error=4.354916631743853\n",
      "2/10, error=4.347501706860016\n",
      "3/10, error=4.339938987060156\n",
      "4/10, error=4.332224601928815\n",
      "5/10, error=4.324354566379513\n",
      "6/10, error=4.316324777455687\n",
      "7/10, error=4.308131011096666\n",
      "8/10, error=4.2997689188756265\n",
      "9/10, error=4.291234024717503\n",
      "10/10, error=4.282521721605986\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "\n",
    "network = [\n",
    "    DenseLayer(2, 3),\n",
    "    Tanh(),\n",
    "    DenseLayer(3, 1),\n",
    "    Tanh()\n",
    "]\n",
    "\n",
    "# train\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "verbose = True\n",
    "for e in range(epochs):\n",
    "    error = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        # forward\n",
    "        output = x \n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        # error\n",
    "        error += meanSquaredError(output, y)\n",
    "\n",
    "        # backward\n",
    "        grad = meanSquaredErrorPrime(output, y)\n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "    error /= len(x)\n",
    "    if verbose:\n",
    "        print(f\"{e + 1}/{epochs}, error={error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIVN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
